{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "from PIL import Image\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# auto. choose CPU or GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Function to load CIFAR-10 dataset\n",
    "def load_cifar_batch(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict_ = pickle.load(fo, encoding='bytes')\n",
    "    return dict_\n",
    "\n",
    "# Specify the directory containing CIFAR-10 batches\n",
    "cifar10_dir = '/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py'\n",
    "\n",
    "# Load metadata (labels)\n",
    "meta_data_dict = load_cifar_batch(os.path.join(cifar10_dir, 'batches.meta'))\n",
    "label_names = [label.decode('utf-8') for label in meta_data_dict[b'label_names']]\n",
    "\n",
    "# Load training data\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for i in range(1, 6):\n",
    "    batch = load_cifar_batch(os.path.join(cifar10_dir, f'data_batch_{i}'))\n",
    "    train_data.append(batch[b'data'])\n",
    "    train_labels += batch[b'labels']\n",
    "\n",
    "train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # Convert to HWC format\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "# Custom dataset\n",
    "class CustomCIFAR10Dataset(torch.utils.data.Dataset):\n",
    "    def _init_(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "train_dataset = CustomCIFAR10Dataset(train_data, train_labels, transform=transform)\n",
    "\n",
    "# (If you want a train/val split from these 50k, uncomment below)\n",
    "# train_size = int(0.9 * len(train_dataset))\n",
    "# val_size = len(train_dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# For now, using the official test set as \"validation\"\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "batch_test_dict = load_cifar_batch(os.path.join(cifar10_dir, 'test_batch'))\n",
    "val_images = batch_test_dict[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "val_labels = np.array(batch_test_dict[b'labels'])\n",
    "\n",
    "val_dataset = CustomCIFAR10Dataset(val_images, val_labels, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load the hidden/unlabeled test set\n",
    "cifar_test_path = '/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl'\n",
    "test_batch = load_cifar_batch(cifar_test_path)\n",
    "test_images = test_batch[b'data'].astype(np.float32) / 255.0\n",
    "\n",
    "test_dataset = [(test_transform(img),) for img in test_images]\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs=50):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 80, 90], gamma=0.1)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss', color='blue')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train Loss & Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracies\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy', color='green')\n",
    "    plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy', color='purple')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Train Accuracy & Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ResidualBlock\n",
    "class ResidualBlock(nn.Module):\n",
    "    def _init_(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self)._init_()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x \n",
    "        if self.skip:\n",
    "            identity = self.skip(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# CustomResNet -> slightly smaller than [40,80,160,320] to get under 5M\n",
    "class CustomResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    layer1: in=38, out=38, 4 blocks\n",
    "    layer2: in=38, out=76, 4 blocks\n",
    "    layer3: in=76, out=152, 3 blocks\n",
    "    layer4: in=152, out=304, 2 blocks\n",
    "    final FC: 304 -> 10\n",
    "    \"\"\"\n",
    "    def _init_(self, num_classes=10):\n",
    "        super(CustomResNet, self)._init_()\n",
    "        self.init_conv = nn.Conv2d(3, 39, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.init_bn = nn.BatchNorm2d(39)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(39, 39, 4, stride=1)\n",
    "        self.layer2 = self._make_layer(39, 78, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(78, 156, 3, stride=2)\n",
    "        self.layer4 = self._make_layer(156, 305, 2, stride=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(305, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        layers = [ResidualBlock(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.init_conv(x)\n",
    "        out = self.init_bn(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = CustomResNet().to(device)\n",
    "\n",
    "# Print summary to verify param count\n",
    "from torchsummary import summary\n",
    "summary(model, (3, 32, 32))\n",
    "\n",
    "# Train\n",
    "train_model(model, train_loader, val_loader, epochs=100)\n",
    "\n",
    "# Submission\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch[0].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "print(\"Submission file saved.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
